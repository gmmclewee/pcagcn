{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aggr class",
      "provenance": [],
      "authorship_tag": "ABX9TyMClF3PHitLdbGP79fDa2BA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmmclewee/pcagcn/blob/main/aggr_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Check if PyTorch Geometric is installed:\n",
        "    import torch_geometric\n",
        "except ImportError:\n",
        "    # If PyTorch Geometric is not installed, install it.\n",
        "    %pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "    %pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "    %pip install -q torch-geometric\n",
        "    %pip install -q torch"
      ],
      "metadata": {
        "id": "Vp85irvZR_Ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sE_l3L-abyC"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch_scatter import scatter, segment_csr\n",
        "\n",
        "from torch_geometric.utils import to_dense_batch\n",
        "\n",
        "\n",
        "class Aggregation(torch.nn.Module, ABC):\n",
        "    r\"\"\"An abstract base class for implementing custom aggregations.\"\"\"\n",
        "    @abstractmethod\n",
        "    def forward(self, x: Tensor, index: Optional[Tensor] = None,\n",
        "                ptr: Optional[Tensor] = None, dim_size: Optional[int] = None,\n",
        "                dim: int = -2) -> Tensor:\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): The source tensor.\n",
        "            index (torch.LongTensor, optional): The indices of elements for\n",
        "                applying the aggregation.\n",
        "                One of :obj:`index` or `ptr` must be defined.\n",
        "                (default: :obj:`None`)\n",
        "            ptr (torch.LongTensor, optional): If given, computes the\n",
        "                aggregation based on sorted inputs in CSR representation.\n",
        "                One of :obj:`index` or `ptr` must be defined.\n",
        "                (default: :obj:`None`)\n",
        "            dim_size (int, optional): The size of the output tensor at\n",
        "                dimension :obj:`dim` after aggregation. (default: :obj:`None`)\n",
        "            dim (int, optional): The dimension in which to aggregate.\n",
        "                (default: :obj:`-2`)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x: Tensor, index: Optional[Tensor] = None,\n",
        "                 ptr: Optional[Tensor] = None, dim_size: Optional[int] = None,\n",
        "                 dim: int = -2) -> Tensor:\n",
        "\n",
        "        if dim >= x.dim() or dim < -x.dim():\n",
        "            raise ValueError(f\"Encountered invalid dimension '{dim}' of \"\n",
        "                             f\"source tensor with {x.dim()} dimensions\")\n",
        "\n",
        "        if index is None and ptr is None:\n",
        "            index = x.new_zeros(x.size(dim), dtype=torch.long)\n",
        "\n",
        "        if ptr is not None:\n",
        "            if dim_size is None:\n",
        "                dim_size = ptr.numel() - 1\n",
        "            elif dim_size != ptr.numel() - 1:\n",
        "                raise ValueError(f\"Encountered invalid 'dim_size' (got \"\n",
        "                                 f\"'{dim_size}' but expected \"\n",
        "                                 f\"'{ptr.numel() - 1}')\")\n",
        "\n",
        "        if index is not None:\n",
        "            if dim_size is None:\n",
        "                dim_size = int(index.max()) + 1 if index.numel() > 0 else 0\n",
        "            elif index.numel() > 0 and dim_size <= int(index.max()):\n",
        "                raise ValueError(f\"Encountered invalid 'dim_size' (got \"\n",
        "                                 f\"'{dim_size}' but expected \"\n",
        "                                 f\">= '{int(index.max()) + 1}')\")\n",
        "\n",
        "        return super().__call__(x, index, ptr, dim_size, dim)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f'{self.__class__.__name__}()'\n",
        "\n",
        "    # Assertions ##############################################################\n",
        "\n",
        "    def assert_index_present(self, index: Optional[Tensor]):\n",
        "        # TODO Currently, not all aggregators support `ptr`. This assert helps\n",
        "        # to ensure that we require `index` to be passed to the computation:\n",
        "        if index is None:\n",
        "            raise NotImplementedError(f\"'{self.__class__.__name__}' requires \"\n",
        "                                      f\"'index' to be specified\")\n",
        "\n",
        "    def assert_sorted_index(self, index: Optional[Tensor]):\n",
        "        if index is not None and not torch.all(index[:-1] <= index[1:]):\n",
        "            raise ValueError(f\"Can not perform aggregation inside \"\n",
        "                             f\"'{self.__class__.__name__}' since the \"\n",
        "                             f\"'index' tensor is not sorted\")\n",
        "\n",
        "    def assert_two_dimensional_input(self, x: Tensor, dim: int):\n",
        "        if x.dim() != 2:\n",
        "            raise ValueError(f\"'{self.__class__.__name__}' requires \"\n",
        "                             f\"two-dimensional inputs (got '{x.dim()}')\")\n",
        "\n",
        "        if dim not in [-2, 0]:\n",
        "            raise ValueError(f\"'{self.__class__.__name__}' needs to perform \"\n",
        "                             f\"aggregation in first dimension (got '{dim}')\")\n",
        "\n",
        "    # Helper methods ##########################################################\n",
        "\n",
        "    def reduce(self, x: Tensor, index: Optional[Tensor] = None,\n",
        "               ptr: Optional[Tensor] = None, dim_size: Optional[int] = None,\n",
        "               dim: int = -2, reduce: str = 'add') -> Tensor:\n",
        "\n",
        "        if ptr is not None:\n",
        "            ptr = expand_left(ptr, dim, dims=x.dim())\n",
        "            return segment_csr(x, ptr, reduce=reduce)\n",
        "\n",
        "        assert index is not None\n",
        "        return scatter(x, index, dim=dim, dim_size=dim_size, reduce=reduce)\n",
        "\n",
        "    def to_dense_batch(self, x: Tensor, index: Optional[Tensor] = None,\n",
        "                       ptr: Optional[Tensor] = None,\n",
        "                       dim_size: Optional[int] = None,\n",
        "                       dim: int = -2) -> Tuple[Tensor, Tensor]:\n",
        "\n",
        "        # TODO Currently, `to_dense_batch` can only operate on `index`:\n",
        "        self.assert_index_present(index)\n",
        "        self.assert_sorted_index(index)\n",
        "        self.assert_two_dimensional_input(x, dim)\n",
        "\n",
        "        return to_dense_batch(x, index, batch_size=dim_size)\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "\n",
        "def expand_left(ptr: Tensor, dim: int, dims: int) -> Tensor:\n",
        "    for _ in range(dims + dim if dim < 0 else dim):\n",
        "        ptr = ptr.unsqueeze(0)\n",
        "    return ptr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.nn import Parameter\n",
        "\n",
        "from torch_geometric.nn.aggr import Aggregation\n",
        "from torch_geometric.utils import softmax\n",
        "\n",
        "class PCAAggregation(Aggregation): # pca based on https://github.com/opensourceai/TensorFlow-PCA/blob/master/pca.ipynb\n",
        "   \n",
        "  def pca(x,dim = 1):\n",
        "    with tf.name_scope(\"PCA\"):\n",
        "        \n",
        "        m,n= tf.to_float(x.get_shape()[0]),tf.to_int32(x.get_shape()[1])\n",
        "        print(n)\n",
        "        assert not tf.assert_less(dim,n)\n",
        "        mean = tf.reduce_mean(x,axis=1)\n",
        "        print(mean)\n",
        "        x_new = x - tf.reshape(mean,(-1,1))\n",
        "        cov = tf.matmul(x_new,x_new,transpose_a=True)/(m - 1) \n",
        "        e,v = tf.linalg.eigh(cov,name=\"eigh\")\n",
        "        e_index_sort = tf.math.top_k(e,sorted=True,k=dim)[1]\n",
        "        v_new = tf.gather(v,indices=e_index_sort)\n",
        "        pca = tf.matmul(x_new,v_new,transpose_b=True)\n",
        "    return pca\n",
        "\n",
        "  def forward(self, x: Tensor, index: Optional[Tensor] = None,\n",
        "                ptr: Optional[Tensor] = None, dim_size: Optional[int] = None,\n",
        "                dim: int = -2) -> Tensor\n",
        "        N_x = Tensor.scatter_(dim, index, x, reduce=None)\n",
        "    return pca(N_x)\n",
        "        \n",
        "     \n",
        "\n",
        "\n",
        "\n",
        "class MeanAggregation(Aggregation):\n",
        "    def forward(self, x: Tensor, index: Optional[Tensor] = None,\n",
        "                ptr: Optional[Tensor] = None, dim_size: Optional[int] = None,\n",
        "                dim: int = -2) -> Tensor:\n",
        "        return self.reduce(x, index, ptr, dim_size, dim, reduce='mean')\n",
        "\n",
        "\n",
        "class SumAggregation(Aggregation):\n",
        "    def forward(self, x: Tensor, index: Optional[Tensor] = None,\n",
        "                ptr: Optional[Tensor] = None, dim_size: Optional[int] = None,\n",
        "                dim: int = -2) -> Tensor:\n",
        "        return self.reduce(x, index, ptr, dim_size, dim, reduce='sum')\n",
        "\n",
        "\n",
        "class MaxAggregation(Aggregation):\n",
        "    def forward(self, x: Tensor, index: Optional[Tensor] = None,\n",
        "                ptr: Optional[Tensor] = None, dim_size: Optional[int] = None,\n",
        "                dim: int = -2) -> Tensor:\n",
        "        return self.reduce(x, index, ptr, dim_size, dim, reduce='max')\n",
        "\n",
        "\n",
        "class MinAggregation(Aggregation):\n",
        "    def forward(self, x: Tensor, index: Optional[Tensor] = None,\n",
        "                ptr: Optional[Tensor] = None, dim_size: Optional[int] = None,\n",
        "                dim: int = -2) -> Tensor:\n",
        "        return self.reduce(x, index, ptr, dim_size, dim, reduce='min')\n",
        "\n",
        "\n",
        "class MulAggregation(Aggregation):\n",
        "    def forward(self, x: Tensor, index: Optional[Tensor] = None,\n",
        "                ptr: Optional[Tensor] = None, dim_size: Optional[int] = None,\n",
        "                dim: int = -2) -> Tensor:\n",
        "        # TODO Currently, `mul` reduction can only operate on `index`:\n",
        "        self.assert_index_present(index)\n",
        "        return self.reduce(x, index, None, dim_size, dim, reduce='mul')\n",
        "\n",
        "\n",
        "class VarAggregation(Aggregation):\n",
        "    def forward(self, x: Tensor, index: Optional[Tensor] = None,\n",
        "                ptr: Optional[Tensor] = None, dim_size: Optional[int] = None,\n",
        "                dim: int = -2) -> Tensor:\n",
        "        mean = self.reduce(x, index, ptr, dim_size, dim, reduce='mean')\n",
        "        mean_2 = self.reduce(x * x, index, ptr, dim_size, dim, reduce='mean')\n",
        "        return mean_2 - mean * mean\n",
        "\n",
        "\n",
        "class StdAggregation(VarAggregation):\n",
        "    def forward(self, x: Tensor, index: Optional[Tensor] = None,\n",
        "                ptr: Optional[Tensor] = None, dim_size: Optional[int] = None,\n",
        "                dim: int = -2) -> Tensor:\n",
        "        var = super().forward(x, index, ptr, dim_size, dim)\n",
        "        return torch.sqrt(var.relu() + 1e-5)\n",
        "\n",
        "\n",
        "class SoftmaxAggregation(Aggregation):\n",
        "    def __init__(self, t: float = 1.0, learn: bool = False):\n",
        "        # TODO Learn distinct `t` per channel.\n",
        "        super().__init__()\n",
        "        self._init_t = t\n",
        "        self.t = Parameter(torch.Tensor(1)) if learn else t\n",
        "        self.learn = learn\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        if isinstance(self.t, Tensor):\n",
        "            self.t.data.fill_(self._init_t)\n",
        "\n",
        "    def forward(self, x: Tensor, index: Optional[Tensor] = None,\n",
        "                ptr: Optional[Tensor] = None, dim_size: Optional[int] = None,\n",
        "                dim: int = -2) -> Tensor:\n",
        "\n",
        "        alpha = x\n",
        "        if not isinstance(self.t, (int, float)) or self.t != 1:\n",
        "            alpha = x * self.t\n",
        "        alpha = softmax(alpha, index, ptr, dim_size, dim)\n",
        "        return self.reduce(x * alpha, index, ptr, dim_size, dim, reduce='sum')\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}(learn={self.learn})')\n",
        "\n",
        "class PowerMeanAggregation(Aggregation):\n",
        "    def __init__(self, p: float = 1.0, learn: bool = False):\n",
        "        # TODO Learn distinct `p` per channel.\n",
        "        super().__init__()\n",
        "        self._init_p = p\n",
        "        self.p = Parameter(torch.Tensor(1)) if learn else p\n",
        "        self.learn = learn\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        if isinstance(self.p, Tensor):\n",
        "            self.p.data.fill_(self._init_p)\n",
        "\n",
        "    def forward(self, x: Tensor, index: Optional[Tensor] = None,\n",
        "                ptr: Optional[Tensor] = None, dim_size: Optional[int] = None,\n",
        "                dim: int = -2) -> Tensor:\n",
        "\n",
        "        out = self.reduce(x, index, ptr, dim_size, dim, reduce='mean')\n",
        "        if isinstance(self.p, (int, float)) and self.p == 1:\n",
        "            return out\n",
        "        return out.clamp_(min=0, max=100).pow(1. / self.p)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}(learn={self.learn})')"
      ],
      "metadata": {
        "id": "NdOMTNTvafKo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}